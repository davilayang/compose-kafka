apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: minikube-zk
  labels: &ZookeeperDeploymentLabels
    app.kubernetes.io/name: minikube
    app.kubernetes.io/component: zookeeper
spec:
  serviceName: minikube-zk-headless
  replicas: 3
  selector:
    matchLabels: *ZookeeperDeploymentLabels # from the labels on line 5
  podManagementPolicy: "Parallel"
  template:
    metadata:
      labels: *ZookeeperDeploymentLabels
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: "app.kubernetes.io/component"
                    operator: In
                    values:
                    - zookeeper
              topologyKey: "kubernetes.io/hostname"
      containers:
      - name: zookeeper
        image: marketplace.gcr.io/google/kafka/zookeeper:2.8
        imagePullPolicy: Never
        # how to pull those images to local? gcloud auth configure-docker, then pull
        # TODO: replace with local built image, local/kafka-base
        resources:
          requests:
            memory: "1250Mi"
            cpu: "300m"
        ports:
        - containerPort: 2181
          name: client
        - containerPort: 2888
          name: server
        - containerPort: 3888
          name: leader-election
        env: # environment variables are processed by start.sh in ConfigMap
        # these variables are for cluster deployment of Zookeeper, i.e. multiple zookeepers for high-availability
        # with single zookeeper, most are not necessary
        - name: JMXAUTH
          value: "false"
        - name: JMXDISABLE
          value: "false"
        - name: JMXPORT
          value: "1099"
        - name: JMXSSL
          value: "false"
        - name : ZK_REPLICAS
          value: "3"
        - name : ZK_HEAP_SIZE
          value: "1000M"
        - name : ZK_TICK_TIME
          value: "2000"
        - name : ZK_MAX_CLIENT_CNXNS
          value: "60"
        - name: ZK_SNAP_RETAIN_COUNT
          value: "3"
        - name: ZK_PURGE_INTERVAL
          value: "24"
        - name: ZK_CLIENT_PORT
          value: "2181"
        - name: ZK_SERVER_PORT
          value: "2888"
        - name: ZK_ELECTION_PORT
          value: "3888"
        - name: ZOO_4LW_COMMANDS_WHITELIST
          value: "*"
        - name: ZK_DATA_DIR
          value: /data
        readinessProbe:
          exec:
            command:
            - /bin/bash
            - -ec
            - |
              echo ruok | nc 127.0.0.1 2181
          initialDelaySeconds: 10
          timeoutSeconds: 5
        livenessProbe:
          exec:
            command:
            - /bin/bash
            - -ec
            - |
              zkServer.sh status
        # zkServer.sh is defined in the GCP image, under /apache-folder/bin
        command:
        - /bin/bash
        - -ec
        - /config-scripts/start.sh # call the script start.sh from ConfigMap to start zookeeper
        volumeMounts:
        - name: config
          mountPath: /config-scripts # config is mounted at /config-scripts subdirectory
        - name: datadir # name of persistent volume claim
          mountPath: /opt/zookeeper  # persistent volume mounted path,
          # but why this path? it's not the dataDir for zookeeper, that is set to /data
          # maybe some mistake from google?
      securityContext:
        runAsUser: 1000
        fsGroup: 1000
      volumes:
        - name: config
          configMap:
            name: minikube-zk-config-scripts # configMap defined by zk-config-script.yaml
            defaultMode: 0555
  volumeClaimTemplates:
  - metadata:
      name: datadir
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: standard
      resources:
        requests:
          storage: 5Gi
